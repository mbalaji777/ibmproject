{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.types import StructField\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "\n",
    "\n",
    "# CREATE SPARK SESSION\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"Kdd cup 1999\").getOrCreate()\n",
    "\n",
    "# build the schema to load dataframe.\n",
    "columnames = [\"duration\", \"protocol_type\", \"service\", \"flag\", \"src_bytes\", \"dst_bytes\", \"land\", \"wrong_fragment\", \"urgent\",\n",
    "      \"hot\", \"num_failed_logins\", \"logged_in\", \"num_compromised\",\n",
    "      \"root_shell\", \"su_attempted\", \"num_root\", \"num_file_creations\",\n",
    "      \"num_shells\", \"num_access_files\", \"num_outbound_cmds\",\n",
    "      \"is_host_login\", \"is_guest_login\", \"count\", \"srv_count\",\n",
    "      \"serror_rate\", \"srv_serror_rate\", \"rerror_rate\", \"srv_rerror_rate\",\n",
    "      \"same_srv_rate\", \"diff_srv_rate\", \"srv_diff_host_rate\",\n",
    "      \"dst_host_count\", \"dst_host_srv_count\",\n",
    "      \"dst_host_same_srv_rate\", \"dst_host_diff_srv_rate\",\n",
    "      \"dst_host_same_src_port_rate\", \"dst_host_srv_diff_host_rate\",\n",
    "      \"dst_host_serror_rate\", \"dst_host_srv_serror_rate\",\n",
    "      \"dst_host_rerror_rate\", \"dst_host_srv_rerror_rate\", \"label\"]\n",
    "\n",
    "schemadata = {}\n",
    "\n",
    "# for i in range(0, len(columnames)):\n",
    "#     print(columnames[i])\n",
    "\n",
    "schemadata[columnames[0]] = FloatType()\n",
    "\n",
    "for i in range(1, 4):\n",
    "    schemadata[columnames[i]] = StringType()\n",
    "\n",
    "for i in range(4, 41):\n",
    "    schemadata[columnames[i]] = FloatType()\n",
    "\n",
    "schemadata[columnames[41]] = StringType()\n",
    "\n",
    "schema = StructType([StructField(key, value, False) for key, value in schemadata.items()])\n",
    "\n",
    "# assembler group all col0...col40 into single column call X\n",
    "#assembler = VectorAssembler(inputCols=columnames[:-1], outputCol=\"features\")\n",
    "assembler = VectorAssembler(inputCols=columnames[:-1], outputCol=\"features\")\n",
    "\n",
    "## TRAINING\n",
    "training = spark.read.csv(\"kddcup_data_10_percent\", schema = schema)\n",
    "\n",
    "indexers = {}\n",
    "\n",
    "# Convert non numeric attributs to numeric for testing dataset\n",
    "for i in range(1, 4):\n",
    "    name = columnames[i]\n",
    "    print(\"Column name: \", name)\n",
    "    indexers[name] = StringIndexer(inputCol=name, outputCol=\"_\"+name)\n",
    "    training = indexers[name].fit(training).transform(training)\n",
    "    training = training.drop(name)\n",
    "    training = training.withColumnRenamed(\"_\"+name, name)\n",
    "\n",
    "# training.printSchema()\n",
    "training = assembler.transform(training) \n",
    "\n",
    "# Keep X and y only\n",
    "#training.select(training.label).show(truncate = False)\n",
    "training = training.withColumn(\"y\", (training.label==\"normal.\").cast(FloatType()))\n",
    "#training.printSchema()\n",
    "training = training.select(\"features\", \"y\")\n",
    "#training.printSchema()\n",
    "training.show()\n",
    "\n",
    "# Logistic Regression\n",
    "lr = LogisticRegression(maxIter=100, regParam=0.03, elasticNetParam=0.8, featuresCol=\"features\", labelCol=\"y\")\n",
    "\n",
    "# Learn a LogisticRegression model using parameters in lr\n",
    "model = lr.fit(training)\n",
    "\n",
    "# ## TESTING\n",
    "testing = spark.read.csv(\"kddcup_data_10_percent\", schema = schema)\n",
    "\n",
    "# Convert non numeric attributs to numeric for testing dataset\n",
    "for i in range(1, 4):\n",
    "    name = columnames[i]\n",
    "    indexers[name] = StringIndexer(inputCol=name, outputCol=\"_\"+name)\n",
    "    testing = indexers[name].fit(testing).transform(testing)\n",
    "    testing = testing.drop(name)\n",
    "    testing = testing.withColumnRenamed(\"_\"+name, name)\n",
    "\n",
    "testing = assembler.transform(testing)\n",
    "\n",
    "# Keep X and y only\n",
    "#testing.select().show(truncate = False)\n",
    "testing = testing.withColumn(\"y\", (testing.label==\"normal.\").cast(FloatType()))\n",
    "#testing.printSchema()\n",
    "testing = testing.select(\"features\", \"y\")\n",
    "# testing.printSchema()\n",
    "# testing.show()\n",
    "\n",
    "# Make prediction\n",
    "prediction = model.transform(testing).select(\"features\", \"y\", \"probability\", \"prediction\")\n",
    "\n",
    "# Show prediction\n",
    "prediction.select(\"y\", \"probability\", \"prediction\").show()\n",
    "\n",
    "# show some Statistics\n",
    "print(\"Show some statistics: \\n\")\n",
    "fraud = prediction.filter(prediction.y == 0).count()\n",
    "fraud_prediction = prediction.filter(prediction.y == 0).filter(prediction.prediction == 0).count()\n",
    "\n",
    "normal = prediction.filter(prediction.y == 1).count()\n",
    "normal_prediction = prediction.filter(prediction.y == 1).filter(prediction.prediction == 1).count()\n",
    "\n",
    "print(\"Number of fraud connections : \" + str(fraud) + \" Number of fraud prediction connections : \" + str(fraud_prediction))\n",
    "print(\"Number of normal connections : \" + str(normal) + \" Number of normal prediction connections : \" + str(normal_prediction))\n",
    "\n",
    "print(\"accuracy on fraud connection: \" + str(fraud_prediction) + \" / \" + str(fraud) + \" = \" + str(float(fraud_prediction) / fraud))\n",
    "print(\"accuracy on normal connection: \" + str(normal_prediction) + \" / \" + str(normal) + \" = \" + str(float(normal_prediction) / normal))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
